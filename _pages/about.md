---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

My name is Junze Yin. I am a second-year Ph.D. student at Rice University, fortunate to be advised by Professor [Maryam Aliakbarpour](https://maryamaliakbarpour.com/). I earned my M.A. and B.A. in Mathematics from Boston University, where I had the privilege of being advised by Professor Mark Kon. My research interests lie at the intersection of theoretical computer science and machine learning. Specifically, I am interested in numerical linear algebra, theoretical machine learning, large language models, and robust statistics.

## [CV](../files/Junze_Yin_2024_PhD_CV_PS.pdf)

# Research

<!-- ICML awaiting for arxiv -->

## Accepted Papers (Author names in alphabetical order)

- Support Basis: Fast Attention Beyond Bounded Entries. ([Article link](https://arxiv.org/pdf/2510.01643))<br>
  Maryam Aliakbarpour, Vladimir Braverman, **Junze Yin**, Haochen Zhang<br>
  AISTATS 2026 (**SPOTLIGHT (Top 3%)**)

- Binary Hypothesis Testing for Softmax Models and Leverage Score Models. ([Article link](https://arxiv.org/pdf/2405.06003)).<br>
  Yuzhou Gu, Zhao Song, and **Junze Yin**<br>
  ICML 2025
- The Expressibility of Polynomial based Attention Scheme. ([Article link](https://arxiv.org/pdf/2310.20051)).<br>
  Zhao Song, Chongxi Wang, Guangyi Xu, and **Junze Yin**<br>
  KDD 2025
- Dynamic maintenance of kernel density estimation data structure: From practice to theory. ([Article link](https://arxiv.org/pdf/2208.03915)).<br>
  Jiehao Liang, Zhao Song, Zhaozhuo Xu, **Junze Yin**, and Danyang Zhuo<br>
  UAI 2025
- A fast optimization view: reformulating single layer attention in LLM based on tensor and svm trick, and solving it in matrix multiplication time. ([Article link](https://arxiv.org/pdf/2309.07418.pdf)).<br>
  Yeqi Gao, Zhao Song, Weixin Wang, and **Junze Yin**<br>
  UAI 2025
- Fast and Efficient Matching Algorithm with Deadline Instances ([Article link](https://arxiv.org/pdf/2305.08353)).<br>
  Zhao Song, Weixin Wang, Chenbo Yin, and **Junze Yin**<br>
  CPAL 2025
- Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation ([Article link](https://arxiv.org/pdf/2306.04169)).<br>
  Zhao Song, Mingquan Ye, **Junze Yin**, and Lichen Zhang<br>
  ICLR 2025
- An Iterative Algorithm for Rescaled Hyperbolic Functions Regression ([Article link](https://arxiv.org/pdf/2305.00660)).<br>
  Yeqi Gao, Zhao Song, and **Junze Yin** <br>
  AISTATS 2025
- Revisiting Quantum Algorithms for Linear Regressions: Quadratic Speedups without Data-Dependent Parameters ([Article link](https://arxiv.org/pdf/2311.14823)).<br>
  Zhao Song, **Junze Yin**, and Ruizhe Zhang<br>
  QIP 2025
- Fast Dynamic Sampling for Determinantal Point Processes ([Article link](https://proceedings.mlr.press/v238/song24b/song24b.pdf)).<br>
  Zhao Song, **Junze Yin**, Lichen Zhang, and Ruizhe Zhang<br>
  AISTATS 2024
- Solving Attention Kernel Regression Problem via Pre-conditioner ([Article link](https://arxiv.org/pdf/2308.14304.pdf)).<br>
  Zhao Song, **Junze Yin**, and Lichen Zhang<br>
  AISTATS 2024
- Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time ([Article link](https://openreview.net/pdf?id=N0gT4A0jNV)).<br>
  Yuzhou Gu, Zhao Song, **Junze Yin**, and Lichen Zhang<br>
  ICLR 2024
- Dynamical fractal: Theory and case study ([Article link](https://doi.org/10.1016/j.chaos.2023.114190)).<br>
  **Junze Yin**<br>
  Chaos, Solitons & Fractals Volume 176, November 2023, 114190.<br>
  Oral Presentation: JMM 2022, AMS Special Session on Geometry in the Mathematics of Data Science<br>
- A Nearly-Optimal Bound for Fast Regression with $\ell_\infty$ Guarantee ([Article link](https://proceedings.mlr.press/v202/song23j/song23j.pdf)).<br>
  Zhao Song, Mingquan Ye, **Junze Yin**, and Lichen Zhang<br>
  ICML, 2023.

---

- Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining ([Article link](https://arxiv.org/pdf/2502.05790)). <br>
  Haochen Zhang, **Junze Yin**, Guanchu Wang, Zirui Liu, Lin F. Yang, Tianyi Zhang, Anshumali Shrivastava, and Vladimir
  Braverman. <br>
  NeurIPS 2025

- CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems ([Article link](https://arxiv.org/pdf/2506.19993)).<br>
  Haochen Zhang, Tianyi Zhang, **Junze Yin**, Oren Gal, Anshumali Shrivastava, and Vladimir Braverman<br>
  ACL 2025 Findings
